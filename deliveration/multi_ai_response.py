from typing import Any, Dict, Optional
import streamlit as st


class MultiAIResponse:
    """
    ãƒãƒ«ãƒAIã®è¿”ç­”ã‚’ç¸¦ã«ä¸¦ã¹ã¦è¡¨ç¤ºã™ã‚‹ãƒ“ãƒ¥ãƒ¼ã‚¢ã®åœŸå°ã‚¯ãƒ©ã‚¹ã€‚

    ãƒ»ã¾ãšã¯ MultiModelViewer ã¨ã»ã¼åŒç­‰ã®æŒ™å‹•
    ãƒ»llm_meta ã‹ã‚‰å„ãƒ¢ãƒ‡ãƒ«ã® reply ã‚’æ‹¾ã£ã¦è¡¨ç¤ºã™ã‚‹
    ãƒ»å°†æ¥ã€JudgeAI ã‚„ Composite ã®æƒ…å ±ã‚’ã“ã“ã«è¶³ã—ã¦ã„ãå‰æ
    """

    def __init__(
        self,
        title: str = "ãƒãƒ«ãƒAIãƒªãƒ—ãƒ©ã‚¤",
    ) -> None:
        self.title = title

        # ã“ã“ã«è¿½åŠ ã—ã¦ã„ãã ã‘ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¢—ã‚„ã›ã‚‹
        # key: llm_meta ã®ã‚­ãƒ¼å / value: è¡¨ç¤ºãƒ©ãƒ™ãƒ«
        self.model_labels: Dict[str, str] = {
            "gpt4o": "GPT-4o",
            "hermes": "Hermes",
            # "claude": "Claude 3" ã¿ãŸã„ã«å¢—ã‚„ã—ã¦ã„ã
        }

    def _extract_model_info(
        self,
        llm_meta: Dict[str, Any],
        key: str,
    ) -> Optional[Dict[str, Any]]:
        """
        llm_meta ã®å½¢ãŒå¤‰ã‚ã£ã¦ã‚‚è€ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã€
        1ã‹æ‰€ã§ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–ã‚Šå‡ºã—æ–¹æ³•ã‚’ã¾ã¨ã‚ã¦ãŠãã€‚

        æƒ³å®šã—ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼š
        1) llm_meta["gpt4o"] = {"reply": "...", ...}
        2) llm_meta["models"]["gpt4o"] = {"reply": "...", ...}
        """

        if key in llm_meta:
            info = llm_meta.get(key)
        else:
            info = llm_meta.get("models", {}).get(key)

        if not isinstance(info, dict):
            return None
        return info

    def render(self, llm_meta: Dict[str, Any] | None) -> None:
        """
        llm_meta ã®ä¸­èº«ã‚’è¦‹ã¦ã€å„ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
        ã¾ã ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãªã„å ´åˆã¯ä½•ã‚‚å‡ºã•ãªã„ã€‚
        """

        # èµ·å‹•ç›´å¾Œãªã© llm_meta ãŒ None / ç©ºdict ã®å ´åˆ
        if not isinstance(llm_meta, dict) or not llm_meta:
            st.caption("ï¼ˆã¾ã ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
            return

        st.markdown(f"### {self.title}")

        # ã‚‚ã— prompt_preview ãŒã‚ã‚Œã°ã€æŠ˜ã‚ŠãŸãŸã¿ã§è¦‹ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãŠã
        prompt_preview = llm_meta.get("prompt_preview")
        if isinstance(prompt_preview, str) and prompt_preview.strip():
            with st.expander("ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                st.code(prompt_preview, language="text")

        has_any = False

        for key, label in self.model_labels.items():
            info = self._extract_model_info(llm_meta, key)
            if not info:
                continue

            has_any = True
            reply = info.get("reply") or info.get("text") or "ï¼ˆè¿”ä¿¡ãªã—ï¼‰"

            st.markdown(f"#### {label}")
            st.write(reply)

            # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ãªã©ãŒã‚ã‚Œã°è»½ãè¡¨ç¤ºï¼ˆã‚ã‚Œã°ã§OKï¼‰
            usage = info.get("usage") or info.get("usage_main")
            if isinstance(usage, dict) and usage:
                prompt_tokens = usage.get("prompt_tokens", "ï¼Ÿ")
                completion_tokens = usage.get("completion_tokens", "ï¼Ÿ")
                total_tokens = usage.get("total_tokens", "ï¼Ÿ")
                st.caption(
                    f"tokens: total={total_tokens}, "
                    f"prompt={prompt_tokens}, completion={completion_tokens}"
                )

            st.markdown("---")

        if not has_any:
            st.caption("ï¼ˆè¡¨ç¤ºå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
