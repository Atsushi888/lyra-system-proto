# views/answertalker_view.py
from __future__ import annotations

from typing import Any, Dict, MutableMapping, Optional

import os
import json
import streamlit as st

from auth.roles import Role  # ã„ã¾ã¯æœªä½¿ç”¨ã ãŒå°†æ¥ã®æ‹¡å¼µç”¨ã«æ®‹ã—ã¦ãŠã
from actors.actor import Actor
from actors.answer_talker import AnswerTalker
from actors.persona.persona_classes.persona_riseria_ja import Persona


# ç’°å¢ƒå¤‰æ•°ã§ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆ
LYRA_DEBUG = os.getenv("LYRA_DEBUG", "0") == "1"


class AnswerTalkerView:
    """
    AnswerTalker / ModelsAI / JudgeAI3 / ComposerAI / MemoryAI ã®
    ãƒ‡ãƒãƒƒã‚°ãƒ»é–²è¦§ç”¨ãƒ“ãƒ¥ãƒ¼ã€‚

    æ³¨æ„ï¼š
    - ã“ã®ãƒ“ãƒ¥ãƒ¼ã¯ã€Œé–²è¦§å°‚ç”¨ã€ã€‚
    - AnswerTalker ã¯ InitAI çµŒç”±ã§ session_state ã‚’è£œä¿®/åˆæœŸåŒ–ã—ã†ã‚‹ãŸã‚ã€
      ã“ã“ã‹ã‚‰ st.session_state ã‚’ AnswerTalker ã«æ¸¡ã™ã¨
      â€œè¦‹ã‚‹ã ã‘ã®ã¤ã‚‚ã‚ŠãŒçŠ¶æ…‹ã‚’æ›¸ãæ›ãˆã‚‹â€ å‰¯ä½œç”¨ãŒèµ·ãã‚‹ã€‚
    - ã—ãŸãŒã£ã¦ AnswerTalker ã«ã¯ãƒ­ãƒ¼ã‚«ãƒ« state ã‚’æ¸¡ã—ã€
      st.session_state ã® llm_meta ã‚’å®‰å…¨ã«é–²è¦§ã™ã‚‹ã€‚
    """

    TITLE = "ğŸ§© AnswerTalkerï¼ˆAIçµ±åˆãƒ†ã‚¹ãƒˆï¼‰"

    @staticmethod
    def _render_any_as_textarea(label: str, value: Any, height: int = 220) -> None:
        """
        llm_meta ã¯éæ¸¡çš„ã«å‹ãŒå¤‰ã‚ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
        - str: text_area
        - dict/list: pretty json ã‚’ text_area
        - ãã®ä»–: str() ã‚’ text_area
        """
        if isinstance(value, str):
            st.text_area(
                label,
                value=value,
                height=height,
                label_visibility="collapsed",
            )
            return

        if isinstance(value, (dict, list)):
            st.text_area(
                label,
                value=json.dumps(value, ensure_ascii=False, indent=2),
                height=height,
                label_visibility="collapsed",
            )
            return

        st.text_area(
            label,
            value="" if value is None else str(value),
            height=height,
            label_visibility="collapsed",
        )

    def __init__(self) -> None:
        # --- ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼åï¼ˆUserSettings ç”±æ¥ï¼‰ã‚’å–å¾— ---
        player_name = st.session_state.get("player_name", "ã‚¢ãƒ„ã‚·")

        # Actor ã¨ AnswerTalker ã‚’åˆæœŸåŒ–
        # Persona ã«ã¯ player_name ã‚’æ¸¡ã—ã¦ãŠã
        persona = Persona(player_name=player_name)
        self.actor = Actor("floria", persona)

        # â˜…é‡è¦ï¼šé–²è¦§å°‚ç”¨ãƒ“ãƒ¥ãƒ¼ãªã®ã§ session_state ã‚’ AnswerTalker ã«æ¸¡ã•ãªã„
        #   â†’ AnswerTalker ç”Ÿæˆã«ã‚ˆã‚‹ session_state ã¸ã®å‰¯ä½œç”¨ã‚’é®æ–­ã™ã‚‹
        local_state: MutableMapping[str, Any] = {}

        self.answer_talker = AnswerTalker(
            persona,
            state=local_state,
        )

    def render(self) -> None:
        st.header(self.TITLE)

        # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã®è»½ã„è¡¨ç¤ºï¼ˆä»»æ„ãƒ»ãƒ‡ãƒãƒƒã‚°è£œåŠ©ï¼‰
        player_name = st.session_state.get("player_name", "ã‚¢ãƒ„ã‚·")
        reply_length_mode = st.session_state.get("reply_length_mode", "auto")
        st.caption(
            f"ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼å: **{player_name}**  /  "
            f"ç™ºè©±é•·ã•ãƒ¢ãƒ¼ãƒ‰: **{reply_length_mode}**"
        )

        st.info(
            "ã“ã®ç”»é¢ã§ã¯ã€Actor ã«ç´ã¥ã AnswerTalker ãŒä¿æŒã—ã¦ã„ã‚‹ llm_meta ã®å†…å®¹ "
            "ï¼ˆsystem_prompt / emotion_override / models / judge / composer / emotion / memoryï¼‰ã‚’å‚ç…§ã§ãã¾ã™ã€‚\n\n"
            "â€» ã“ã®ç”»é¢ã‹ã‚‰ã¯ AnswerTalker.run_models() ã‚„ MemoryAI.update_from_turn() ãªã©ã¯å®Ÿè¡Œã—ã¾ã›ã‚“ã€‚"
        )

        llm_meta: Dict[str, Any] = st.session_state.get("llm_meta", {}) or {}

        # ---- ä»Šå›ä½¿ç”¨ã•ã‚ŒãŸ system_prompt ----
        st.subheader("ä»Šå›ä½¿ç”¨ã•ã‚ŒãŸ system_promptï¼ˆaffection / ãƒ‰ã‚­ãƒ‰ã‚­ğŸ’“åæ˜ å¾Œï¼‰")

        if "system_prompt_used" not in llm_meta:
            st.info("system_prompt_used ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚ï¼ˆã‚­ãƒ¼æœªä½œæˆï¼‰")
        else:
            sys_used = llm_meta.get("system_prompt_used")
            st.caption(
                f"system_prompt_used type={type(sys_used).__name__} / "
                f"len={len(sys_used) if isinstance(sys_used, str) else '(n/a)'}"
            )
            # ç©ºæ–‡å­—ã§ã‚‚ã€Œç©ºãŒå…¥ã£ã¦ã„ã‚‹ã€ã“ã¨è‡ªä½“ãŒé‡è¦ãªã®ã§å¿…ãšå‡ºã™
            self._render_any_as_textarea("system_prompt_used", sys_used, height=220)

        # ---- emotion_override ----
        st.subheader("emotion_overrideï¼ˆMixerAI â†’ ModelsAI ã«æ¸¡ã—ãŸæ„Ÿæƒ…ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰")
        emo_override = llm_meta.get("emotion_override") or {}
        if not emo_override:
            st.info("emotion_override ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.json(emo_override)

        # ---- models ----
        st.subheader("llm_meta ã«ç™»éŒ²ã•ã‚ŒãŸ AI å›ç­”ä¸€è¦§ï¼ˆmodelsï¼‰")
        models = llm_meta.get("models", {})

        if not models:
            st.info("models æƒ…å ±ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            for name, info in models.items():
                with st.expander(f"ãƒ¢ãƒ‡ãƒ«: {name}", expanded=True):
                    status = info.get("status", "unknown")
                    text = info.get("text", "") or ""
                    usage = info.get("usage")
                    error = info.get("error")

                    st.write("- status:", status)
                    st.write("- len(text):", len(text))

                    if usage is not None:
                        st.write("- usage:", usage)

                    if error:
                        st.error(f"error: {error}")

                    if text:
                        st.markdown("**preview:**")
                        st.code(text[:1000])

        # ---- judge ----
        st.subheader("JudgeAI3 ã®åˆ¤å®šçµæœï¼ˆllm_meta['judge']ï¼‰")
        judge = llm_meta.get("judge", {})
        if not judge:
            st.info("judge æƒ…å ±ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.write(f"- status: `{judge.get('status', 'unknown')}`")
            st.write(f"- chosen_model: `{judge.get('chosen_model', '')}`")

            reason = judge.get("reason")
            if reason:
                with st.expander("é¸æŠç†ç”±ï¼ˆreasonï¼‰", expanded=True):
                    st.write(reason)

            chosen_text = (judge.get("chosen_text") or "").strip()
            if chosen_text:
                with st.expander("æ¡ç”¨ãƒ†ã‚­ã‚¹ãƒˆï¼ˆchosen_textï¼‰", expanded=True):
                    st.text_area(
                        "chosen_text",
                        value=chosen_text,
                        height=260,
                        label_visibility="collapsed",
                    )

            raw_candidates = judge.get("candidates") or []
            with st.expander("å€™è£œãƒ¢ãƒ‡ãƒ«ä¸€è¦§ï¼ˆcandidates / scoresï¼‰", expanded=False):
                if isinstance(raw_candidates, dict):
                    for cand_name, cand_info in raw_candidates.items():
                        score = cand_info.get("score", "-")
                        preview = (cand_info.get("text") or "")[:800]
                        st.markdown(f"### {cand_name}  |  score = `{score}`")
                        st.write(preview)
                        st.markdown("---")
                elif isinstance(raw_candidates, list):
                    for i, cand in enumerate(raw_candidates, start=1):
                        cand_name = cand.get("name", f"cand-{i}")
                        score = cand.get("score", "-")
                        length = cand.get("length", 0)
                        preview = (cand.get("text") or "")[:800]
                        st.markdown(
                            f"### å€™è£œ {i}: `{cand_name}`  |  score = `{score}`  |  length = {length}"
                        )
                        details = cand.get("details") or {}
                        if details:
                            with st.expander("details", expanded=False):
                                st.json(details)
                        st.markdown("---")
                else:
                    st.write("candidates ã®å½¢å¼ãŒæƒ³å®šå¤–ã§ã™:", type(raw_candidates))

        # ---- composer ----
        st.subheader("ComposerAI ã®æœ€çµ‚çµæœï¼ˆllm_meta['composer']ï¼‰")
        comp = llm_meta.get("composer", {})
        if not comp:
            st.info("composer æƒ…å ±ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.write(f"- status: `{comp.get('status', 'unknown')}`")
            st.write(f"- source_model: `{comp.get('source_model', '')}`")
            st.write(f"- mode: `{comp.get('mode', '')}`")

            base_src = comp.get("base_source_model")
            if base_src:
                st.write(f"- base_source_model: `{base_src}`")

            dev_force = comp.get("dev_force_model")
            if dev_force:
                st.write(f"- dev_force_model: `{dev_force}`")

            st.write(f"- is_modified: `{comp.get('is_modified', False)}`")

            summary = comp.get("summary")
            if summary:
                with st.expander("ã‚µãƒãƒªï¼ˆsummaryï¼‰", expanded=True):
                    st.text_area(
                        "composer_summary",
                        value=str(summary),
                        height=200,
                        label_visibility="collapsed",
                    )

            # Refiner æƒ…å ±
            with st.expander("Refiner æƒ…å ±", expanded=False):
                st.write(f"- refiner_model: `{comp.get('refiner_model', None)}`")
                st.write(f"- refiner_used: `{comp.get('refiner_used', False)}`")
                st.write(f"- refiner_status: `{comp.get('refiner_status', '')}`")
                ref_err = comp.get("refiner_error")
                if ref_err:
                    st.error(f"refiner_error: {ref_err}")

            # base_text / æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆæ¯”è¼ƒ
            base_text = (comp.get("base_text") or "").strip()
            final_text = (comp.get("text") or "").strip()

            if base_text:
                with st.expander("Refiner å‰ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆbase_textï¼‰", expanded=False):
                    st.text_area(
                        "composer_base_text",
                        value=base_text,
                        height=260,
                        label_visibility="collapsed",
                    )

            if final_text:
                with st.expander("æœ€çµ‚è¿”ç­”ãƒ†ã‚­ã‚¹ãƒˆï¼ˆcomposer.textï¼‰", expanded=True):
                    st.text_area(
                        "composer_text",
                        value=final_text,
                        height=260,
                        label_visibility="collapsed",
                    )

        # ---- Composer ç”¨ã‚¹ã‚¿ã‚¤ãƒ«ãƒ’ãƒ³ãƒˆï¼ˆpersona ç”±æ¥ï¼‰ ----
        style_hint = llm_meta.get("composer_style_hint") or ""
        if style_hint:
            st.subheader("Composer ç”¨ã‚¹ã‚¿ã‚¤ãƒ«ãƒ’ãƒ³ãƒˆï¼ˆpersona ç”±æ¥ï¼‰")
            with st.expander("composer_style_hint", expanded=False):
                st.text_area(
                    "composer_style_hint",
                    value=style_hint,
                    height=260,
                    label_visibility="collapsed",
                )

        # ---- Judge ãƒ¢ãƒ¼ãƒ‰çŠ¶æ…‹ ----
        st.subheader("Judge ãƒ¢ãƒ¼ãƒ‰çŠ¶æ…‹")
        current_mode_meta = llm_meta.get("judge_mode", None)
        next_mode_meta = llm_meta.get("judge_mode_next", None)
        session_mode = st.session_state.get("judge_mode", None)

        cols_mode = st.columns(3)
        with cols_mode[0]:
            st.write(f"llm_meta['judge_mode']: `{current_mode_meta}`")
        with cols_mode[1]:
            st.write(f"llm_meta['judge_mode_next']: `{next_mode_meta}`")
        with cols_mode[2]:
            st.write(f"session_state['judge_mode']: `{session_mode}`")

        # ---- EmotionAI ----
        st.subheader("EmotionAI ã®è§£æçµæœï¼ˆllm_meta['emotion']ï¼‰")

        emo = llm_meta.get("emotion") or {}
        emo_err = llm_meta.get("emotion_error")

        if emo_err:
            st.error(f"EmotionAI error: {emo_err}")

        if not emo:
            st.info("Emotion æƒ…å ±ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.markdown(f"- æ¨å®š judge_mode: `{emo.get('mode', 'normal')}`")

            cols = st.columns(3)
            with cols[0]:
                st.write(f"affection: {emo.get('affection', 0.0):.2f}")
                st.write(f"arousal:   {emo.get('arousal', 0.0):.2f}")
            with cols[1]:
                st.write(f"tension:   {emo.get('tension', 0.0):.2f}")
                st.write(f"anger:     {emo.get('anger', 0.0):.2f}")
            with cols[2]:
                st.write(f"sadness:   {emo.get('sadness', 0.0):.2f}")
                st.write(f"excitement:{emo.get('excitement', 0.0):.2f}")

            with st.expander("raw_textï¼ˆEmotionAI ã® LLM å‡ºåŠ›ï¼‰", expanded=False):
                st.code(emo.get("raw_text", ""), language="json")

        # ---- MemoryAI ----
        st.subheader("MemoryAI ã®çŠ¶æ…‹ï¼ˆé•·æœŸè¨˜æ†¶ï¼‰")
        memory_ctx = llm_meta.get("memory_context") or ""
        mem_update = llm_meta.get("memory_update") or {}

        memory_ai = getattr(self.answer_talker, "memory_ai", None)

        if memory_ai is None:
            st.warning("AnswerTalker.memory_ai ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        else:
            persona_id = getattr(memory_ai, "persona_id", "default")
            max_records = getattr(memory_ai, "max_store_items", 0)
            storage_file = getattr(memory_ai, "file_path", "(unknown)")
            st.write(f"- persona_id: `{persona_id}`")
            st.write(f"- max_records: `{max_records}`")
            st.write(f"- storage_file: `{storage_file}`")

            try:
                records = memory_ai.get_all_records()
            except Exception as e:
                records = []
                st.warning(f"MemoryRecord ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

            if not records:
                st.info("ç¾åœ¨ã€ä¿å­˜æ¸ˆã¿ã® MemoryRecord ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.markdown("#### ä¿å­˜æ¸ˆã¿ MemoryRecord ä¸€è¦§")
                for i, r in enumerate(records, start=1):
                    with st.expander(
                        f"è¨˜æ†¶ {i}: [imp={r.importance}] {r.summary[:32]}...",
                        expanded=False,
                    ):
                        st.write(f"- id: `{r.id}`")
                        st.write(f"- round_id: {r.round_id}")
                        st.write(f"- importance: {r.importance}")
                        st.write(f"- created_at: {r.created_at}")
                        st.write(
                            f"- tags: {', '.join(r.tags) if r.tags else '(ãªã—)'}"
                        )
                        st.write("**summary:**")
                        st.write(r.summary)
                        if r.source_user:
                            st.write("\n**source_user:**")
                            st.text(r.source_user)
                        if r.source_assistant:
                            st.write("\n**source_assistant:**")
                            st.text(r.source_assistant)

            st.markdown("---")
            st.markdown("### MemoryAI ãƒ•ã‚¡ã‚¤ãƒ«è¨ºæ–­ï¼ˆJSONï¼‰")

            if st.button("è¨˜æ†¶ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨ºæ–­ã™ã‚‹", key="memfile_check_at"):
                path = storage_file
                st.write(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: `{path}`")

                if not path or path == "(unknown)":
                    st.error("MemoryAI.file_path ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                elif not os.path.exists(path):
                    st.error(
                        "ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã¾ã ä¸€åº¦ã‚‚è¨˜æ†¶ãŒä¿å­˜ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                    )
                else:
                    st.success("ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã—ã¾ã™ã€‚")

                    size = os.path.getsize(path)
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: `{size}` ãƒã‚¤ãƒˆ")

                    try:
                        with open(path, "r", encoding="utf-8") as f:
                            data = json.load(f)
                    except Exception as e:
                        st.error(f"JSON ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    else:
                        if isinstance(data, list):
                            st.write(f"- JSON ã¯ãƒªã‚¹ãƒˆã§ã™ã€‚è¦ç´ æ•°: `{len(data)}`")
                            if data:
                                st.write("- å…ˆé ­3ä»¶ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
                                st.json(data[:3])
                            else:
                                st.info("ãƒªã‚¹ãƒˆã¯ç©ºã§ã™ï¼ˆè¨˜æ†¶ãŒ 0 ä»¶ã§ã™ï¼‰ã€‚")
                        else:
                            st.write(f"- JSON ã®å‹: `{type(data)}`")
                            st.json(data)

        st.subheader("llm_meta å†…ã®ãƒ¡ãƒ¢ãƒªé–¢é€£ãƒ¡ã‚¿æƒ…å ±")
        st.write(f"- memory_context:\n\n```text\n{memory_ctx}\n```")
        st.write("- memory_updateï¼ˆç›´è¿‘ã‚¿ãƒ¼ãƒ³ã®è¨˜æ†¶æ›´æ–°çµæœï¼‰:")
        st.json(mem_update)


def create_answertalker_view() -> AnswerTalkerView:
    """
    ModeSwitcher ã‹ã‚‰å‘¼ã¶ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°ã€‚
    """
    return AnswerTalkerView()
