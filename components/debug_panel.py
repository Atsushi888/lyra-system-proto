# components/debug_panel.py

from __future__ import annotations

from typing import Any, Dict
import json

import streamlit as st

from deliberation.multi_ai_response import MultiAIResponse


class DebugPanel:
    """
    LLMå‘¼ã³å‡ºã—ã®ãƒ¡ã‚¿æƒ…å ±ã¨ã€
    ãƒãƒ«ãƒAIå¯©è­°ã®çµæœï¼ˆJudgeï¼‰ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤ºã™ã‚‹ãƒ‘ãƒãƒ«ã€‚
    """

    def __init__(self, title: str = "Debug Panel") -> None:
        self.title = title
        self.multi_ai = MultiAIResponse()

        # ã“ã“ã«è¿½åŠ ã—ã¦ã„ãã ã‘ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¢—ã‚„ã›ã‚‹
        # key: llm_meta["models"] ã®ã‚­ãƒ¼å / value: è¡¨ç¤ºãƒ©ãƒ™ãƒ«
        self.model_labels: Dict[str, str] = {
            "gpt4o": "GPT-4o",
            "hermes": "Hermes",
            # "claude": "Claude 3" ãªã©å¢—ã‚„ã—ã¦OK
        }

    # ===== å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ï¼šãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ“ãƒ¥ãƒ¼ï¼ˆæ—§ MultiModelViewer ã®å¾©æ´»ç‰ˆï¼‰ =====
    def _render_model_compare(self, models: Dict[str, Any]) -> None:
        st.markdown("#### ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")

        has_any = False
        for key, label in self.model_labels.items():
            info = models.get(key)
            if not isinstance(info, dict):
                continue

            has_any = True
            reply = info.get("reply") or info.get("text") or "ï¼ˆè¿”ä¿¡ãªã—ï¼‰"

            st.markdown(f"**{label}**")
            st.write(reply)

            usage = info.get("usage") or info.get("usage_main")
            if isinstance(usage, dict) and usage:
                pt = usage.get("prompt_tokens", "ï¼Ÿ")
                ct = usage.get("completion_tokens", "ï¼Ÿ")
                tt = usage.get("total_tokens", "ï¼Ÿ")
                st.caption(
                    f"tokens: total={tt}, prompt={pt}, completion={ct}"
                )

            st.markdown("---")

        if not has_any:
            st.caption("ï¼ˆè¡¨ç¤ºå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")

    # ===== å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ï¼šJudgeçµæœè¡¨ç¤º =====
    def _render_multi_ai_result(self, judge: Dict[str, Any] | None) -> None:
        st.markdown("#### âš–ï¸ Multi AI Judge")

        if not isinstance(judge, dict):
            st.caption("ï¼ˆå¯©è­°çµæœã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ï¼‰")
            return

        winner = judge.get("winner", "ï¼Ÿ")
        score = judge.get("score_diff", 0.0)
        comment = judge.get("comment", "")

        cols = st.columns(2)
        cols[0].metric("å‹è€…", winner)
        cols[1].metric(
            "ã‚¹ã‚³ã‚¢å·®",
            f"{score:.2f}" if isinstance(score, (int, float)) else score,
        )

        if comment:
            st.markdown("**ç†ç”±:**")
            st.write(comment)

        with st.expander("ğŸª¶ JudgeAI raw", expanded=False):
            raw = judge.get("raw")
            if raw:
                st.code(str(raw), language="text")
            pair = judge.get("pair")
            if pair:
                st.caption(f"æ¯”è¼ƒãƒšã‚¢: {pair}")

    # ===== å…¬é–‹ï¼šæç”»æœ¬ä½“ =====
    def render(self, llm_meta: Dict[str, Any] | None) -> None:
        st.markdown(f"### ğŸ›  {self.title}")

        if not isinstance(llm_meta, dict) or not llm_meta:
            st.caption("ï¼ˆã¾ã ãƒ¡ã‚¿æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
            return

        # --- åŸºæœ¬æƒ…å ± ---
        route = llm_meta.get("route")
        model_main = llm_meta.get("model_main")
        usage_main = llm_meta.get("usage_main") or llm_meta.get("usage")

        with st.expander("åŸºæœ¬æƒ…å ±", expanded=False):
            if route:
                st.write(f"- route: `{route}`")
            if model_main:
                st.write(f"- model_main: `{model_main}`")
            if isinstance(usage_main, dict):
                pt = usage_main.get("prompt_tokens", "ï¼Ÿ")
                ct = usage_main.get("completion_tokens", "ï¼Ÿ")
                tt = usage_main.get("total_tokens", "ï¼Ÿ")
                st.write(f"- tokens: total={tt}, prompt={pt}, completion={ct}")

        # --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ---
        prompt_preview = llm_meta.get("prompt_preview")
        if isinstance(prompt_preview, str) and prompt_preview.strip():
            with st.expander("ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                st.code(prompt_preview, language="text")

        # --- models ã‚’ä½¿ã£ãŸ GPT-4o / Hermes æ¯”è¼ƒ ---
        models = llm_meta.get("models")
        if isinstance(models, dict) and models:
            with st.expander("ğŸ¤ ãƒ¢ãƒ‡ãƒ«å¿œç­”æ¯”è¼ƒ", expanded=True):
                self._render_model_compare(models)

        # --- MultiAIResponse ã«ã‚ˆã‚‹ Judge çµæœã‚’è¡¨ç¤º ---
        #     ï¼ˆã“ã“ã§ JudgeAI ã‚’å‘¼ã¶ã®ã§ã¯ãªãã€multi_ai_response ã«ä»»ã›ã‚‹ï¼‰
        agg = self.multi_ai.process(llm_meta)
        judge = agg.get("judge")

        with st.expander("âš–ï¸ ãƒãƒ«ãƒAIå¯©è­°çµæœ", expanded=True):
            self._render_multi_ai_result(judge)

        # --- ç”Ÿ llm_meta ã‚’æœ€å¾Œã«ç½®ã„ã¦ãŠã ---
        with st.expander("raw llm_meta (é–‹ç™ºè€…å‘ã‘)", expanded=False):
            try:
                st.code(
                    json.dumps(llm_meta, ensure_ascii=False, indent=2),
                    language="json",
                )
            except Exception:
                st.code(str(llm_meta), language="text")
